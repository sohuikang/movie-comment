#Mecab 형태소 분석기 설치
# ! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
# ! ls
# % cd Mecab-ko-for-Google-Colab
# ! bash install_mecab-ko_on_colab190912.sh

#나눔 폰트 설치
# !sudo apt-get install -y fonts-nanum
# !sudo fc-cache -fv
# !rm ~/.cache/matplotlib -rf

#Matplotlib 폰트 다시 로드하기
# from matplotlib import font_manager as fm
# fm._rebuild()

#패키지 준비하기
import json
import math
import numpy as np
import seaborn as sns
from pandas import DataFrame
from pandas import read_excel
from pandas import read_table
from konlpy.tag import Mecab
from matplotlib import pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, GRU, LSTM, SimpleRNN
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

#데이터셋 준비하기
df = read_excel('http://itpaper.co.kr/data/movie_review.xlsx', engine='openpyxl')
df

#데이터 전처리
df['label'] = df['label'].replace({'긍정': 1, '부정': 0})
df['label'].unique()

#결측치 확인
df.isna().sum()

#결측치 삭제
df.dropna(inplace=True)
df.isna().sum()

#긍정과 부정의 비율 확인
value_counts = df['label'].value_counts()
value_counts

#불용어 목록 로드

swdf = read_table('http://itpaper.co.kr/data/korean_stopwords_100.txt',
                  encoding='utf-8', sep='\s+',
                  names=['불용어', '품사', '비율'])
stopwords = list(swdf['불용어'])
print(stopwords)

#형태소 분석 및 불용어 제거하기
mecab = Mecab()
word_set = []
for i, v in enumerate(df['document']):
    morphs = mecab.morphs(v)
    tmp_word = []
    for j in morphs:
        if j not in stopwords:
            tmp_word.append(j)
    word_set.append(tmp_word)
# 상위 3건만 출력해서 확인
print(word_set[:3])

#말뭉치 토큰화
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(word_set)
print(f'전체 단어수: {len(tokenizer.word_index)}')

#자주 등장하는 단어의 수 구하기
threshold = 3 # 사용 빈도가 높다고 판단할 등장 회수
total_cnt = len(tokenizer.word_index) # 전체 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트할 값
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value
    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value
print('단어 집합(vocabulary)의 크기 :',total_cnt)
print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print("단어 집합에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)

# 자주 등장하는 단어 집합의 크기 구하기 -> 이 값이 첫 번째 학습층의 input 수가 된다.
vocab_size = total_cnt - rare_cnt + 2
print('단어 집합의 크기 :', vocab_size)


#자주 등장하는 단어를 제외한 나머지 단어를 oov로 처리하여 최종 토큰화 진행
tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')
tokenizer.fit_on_texts(word_set)
token_set = tokenizer.texts_to_sequences(word_set)
print('토큰의 크기 :', len(token_set))

with open("word_index.json", "w", encoding='utf-8') as f:
json = json.dumps(tokenizer.word_index)
f.write(json)

# 토큰화 결과 길이가 0인 항목의 index 찾기
drop_target_index = []
for i, v in enumerate(token_set):
    if len(v) < 1:
        drop_target_index.append(i)
print("길이가 0인 항목의 수: ", len(drop_target_index))

kkk = df['label'].values
len(kkk)

# 토큰 결과에서 해당 위치의 항목들을 삭제한다.
fill_token_set = np.delete(token_set, drop_target_index, axis=0)

# 종속변수에서도 같은 위치의 항목들을 삭제해야 한다.
labels = np.delete(kkk, drop_target_index, axis=0)

#각 문장별로 몇 개의 단어를 포함하고 있는지 측정 
word_counts = []
for s in fill_token_set:
    word_counts.append(len(s))
print(word_counts)

#댓글 중 가장 많은 단어를 사용한 리뷰와 가장 적은 단어를 사용한 리뷰의 단어 수
max_word_count = max(word_counts)
min_word_count = min(word_counts)
print('리뷰의 최대 단어수 :',max_word_count)
print('리뷰의 최대 단어수 :',min_word_count)

# 히스토그램의 범위 산정
range_step = 10
bin_step = 20
min_value = (min_word_count//range_step)*range_step
max_value = ((max_word_count+5)//range_step)*range_step
b = math.ceil((max_value-min_value)/bin_step)
hist_values, hist_bins = np.histogram(word_counts, range=(min_value, max_value), bins=b)
hist_bins = hist_bins.astype(np.int64)
# 그래프 초기화
plt.rcParams["font.family"] = 'NanumGothic'
plt.rcParams["font.size"] = 16
plt.rcParams['axes.unicode_minus'] = False
# 히스토그램 시각화
fig, ax = plt.subplots(1, 1, figsize=(10, 5), dpi=100)
sns.histplot(word_counts, bins=b, binrange=(min_value, max_value), ax=ax)
ax.set_xlabel('샘플내 단어 수')
ax.set_ylabel('리뷰수')
ax.set_xticks(hist_bins)
ax.set_xticklabels(hist_bins)
# 수치값 텍스트 출력
for i, v in enumerate(hist_values):
    x = hist_bins[i] + ((hist_bins[i+1] - hist_bins[i])/2)
    ax.text(x=x, y=v, s=str(v), fontsize=12, verticalalignment='bottom', horizontalalignment='center')
plt.savefig('hist.png', dpi=200, bbox_inches='tight')
plt.show()
plt.close()

#패딩처리
pad_token_set = pad_sequences(fill_token_set, maxlen=max_word_count)
pad_token_set

#데이터셋 분할하기
np.random.seed(777)

#데이터 분할
x_train, x_test, y_train, y_test = train_test_split(
pad_token_set, labels, test_size = 0.3, random_state = 777)
print("훈련용 데이터셋 크기: %s, 검증용 데이터셋 크기: %s" % (x_train.shape, x_test.shape))
print("훈련용 레이블 크기: %s, 검증용 레이블 크기: %s" % (y_train.shape, y_test.shape))

#종속변수 one-hot-encoding
y_train_one_hot = to_categorical(y_train)
y_test_one_hot = to_categorical(y_test)

#모델 개발
model = Sequential()
# input_dim의 크기는 토큰 생성시 지정한 최대 단어수(vocab_size)와 동일하게 설정
# output_dim의 크기는 input_dim보다 작은 값 중에서 설정
model.add(Embedding(input_dim = vocab_size, output_dim = 32))
model.add(GRU(128))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())
# 학습하기
result = model.fit(x_train, y_train, batch_size = 10, epochs = 30,
                  validation_data=(x_test, y_test), callbacks = [
    EarlyStopping(monitor = 'val_loss', patience=5, verbose = 1),
    ReduceLROnPlateau(monitor= "val_loss", patience=3, factor = 0.5, min_lr=0.0001, verbose=1)
])
# 학습결과 표
result_df = DataFrame(result.history)
result_df['epochs'] = result_df.index+1
result_df.set_index('epochs', inplace=True)
result_df

#학습 과정 시각화
# 그래프 기본 설정
# ----------------------------------------
plt.rcParams["font.family"] = 'NanumGothic'
plt.rcParams["font.size"] = 16
plt.rcParams['axes.unicode_minus'] = False
# 그래프를 그리기 위한 객체 생성
# ----------------------------------------
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5), dpi=150)
# 1) 훈련 및 검증 손실 그리기
# ----------------------------------------
sns.lineplot(x=result_df.index, y='loss', data=result_df, color='blue',
            label='훈련 손실률', ax=ax1)
sns.lineplot(x=result_df.index, y='val_loss', data=result_df, color='orange', label='검증 손실률', ax=a
ax1.set_title('훈련 및 검증 손실률')
ax1.set_xlabel('반복회차')
ax1.set_ylabel('손실률')
ax1.grid()
ax1.legend()
# 2) 훈련 및 검증 정확도 그리기
# ----------------------------------------
sns.lineplot(x=result_df.index, y='acc', data=result_df, color = 'blue',
            label = '훈련 정확도', ax=ax2)
sns.lineplot(x=result_df.index, y='val_acc', data=result_df, color = 'orange', label = '검증 정확도', a
ax2.set_title('훈련 및 검증 정확도')
ax2.set_xlabel('반복회차')
ax2.set_ylabel('정확도')
ax2.grid()
ax2.legend()
plt.savefig('result.png', dpi=200, bbox_inches='tight')
plt.show()
plt.close()

#훈련, 검증 정확도 평가
train_evaluate = model.evaluate(x_train, y_train)
print("최종 훈련 손실률: %f, 최종 훈련 정확도: %f" % (train_evaluate[0], train_evaluate[1]))
test_evaluate = model.evaluate(x_test, y_test)
print("최종 검증 손실률: %f, 최종 검증 정확도: %f" % (test_evaluate[0], test_evaluate[1]))

#학습결과 적용
result = model.predict(x_test)
data_count, case_count = result.shape
print("%d개의 검증 데이터가 %d개의 경우의 수를 갖는다." % (data_count, case_count))
kdf = DataFrame({
    '결과값': y_test,
    '예측치' : np.round(result.flatten())
})
kdf['예측치'] = kdf['예측치'].astype('int')
kdf

#오차행렬 히트맵
cm = confusion_matrix(kdf['결과값'], kdf['예측치'])
cmdf1 = DataFrame(cm, columns=['예측값(P)', '예측값(N)'], index=['실제값(T)', '실제값(F)'])
plt.rcParams["font.family"] = 'NanumGothic'
plt.rcParams["font.size"] = 20
fig, ax = plt.subplots(1, 1, figsize=(10, 5))
sns.heatmap(cm, annot = True, fmt = 'd',cmap = 'Blues', ax=ax)
ax.set_xlabel('예측값')
ax.set_ylabel('결과값')
plt.savefig('heatmap.png', dpi=200, bbox_inches='tight')
plt.show()
plt.close()

#임의의 문장 5개에 대한 분류
comment = [
          "오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 유쾌함",
          "킬링타임용으로 괜찮은 영화였다.",
          "뭔가 한방이 없어서 아쉬웠던 영화. 또 보고 싶지는 않다.",
          "절대 봐서는 안 될 영화.",
          "감독의 유머감각이 나와 잘 맞는듯 하다. 유쾌하게 봤다."
]
mecab = Mecab()
word_set = []
for c in comment:
    morphs = mecab.morphs(c)
    tmp_word = []
    for j in morphs:
        if j not in stopwords:
            tmp_word.append(j)
    word_set.append(tmp_word)
# 자주 등장하는 단어를 제외한 나머지 단어를 OOV로 처리하여 최종 토큰화 수행
tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')
tokenizer.fit_on_texts(word_set)
token_set = tokenizer.texts_to_sequences(word_set)
pad_token_set = pad_sequences(token_set, maxlen=max_word_count)
result = model.predict(pad_token_set)
for i, v in enumerate(result.flatten()):
    p = np.round(v * 100, 1)
    if p > 50:
      print("%.1f%%의 확률로 긍정입니다. >> %s" % (p, comment[i]))
    else:
      print("%.1f%%의 확률로 부정입니다. >> %s" % (100-p, comment[i]))
